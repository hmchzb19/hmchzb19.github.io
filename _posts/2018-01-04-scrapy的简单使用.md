- scrapy 是一个强大的爬虫工具

0. 安装: 在Kali linux 下安装很方便。

   
    apt-get update<p>
    apt-get install python3-scrapy
    

1. scrapy命令基本使用如下:



        scrapy startproject myproject<p>
        #创建一个project
        scrapy genspider wikiSpider en.wikipedia.org
        #生成一个spider
        scrapy list
        #列出所有的spider
        scrapy shell
        #进入scrapy shell
        #scrapy shell 后面可以直接跟上URL例如：
        scrapy shell chinaunix.net
        #在scrapy shell里面可以使用TAB来自动补全　
        例如：
        In [3]: response.body<p>
   response.body            response.flags           response.request          
   response.body_as_unicode response.follow          response.selector         
   response.copy            response.headers         response.status          >
   response.css             response.meta            response.text             
   response.encoding        response.replace         response.url  

2. 感觉scrapy shell 非常好用，response 就是你需要parse的内容了。通常用XPATH来取所需要的数据，例如.

        In [5]: response.xpath('//title/text()').extract_first()
        Out[5]: 'Unix技术网 = 全球最大的Linux/Unix应用与开发者社区 = IT人的网上家园'
        #下面的这一行，可以取到所有的Ａ标签里href有chinaunix字段的a标签的href attribute,意味着我可以取到这个webpage上所有链接的URL.
        In [9]: response.xpath('//a[contains(@href,"chinaunix")]/@href').extract()
        #extract() 返回是一个列表，extract_first()会返回列表的第一项，其实就是 list[0].
        response.xpath('//a[contains(@href,"chinaunix")]/@href').extract_first() 

3. 其他的一些命令行



        #爬给定的URL或者网站的命令简单粗暴
        scrapy crawl [SPIDERNAME]
        #可以通过-s 传入参数,例如
        scrapy crawl apts -s LOG_FILE=wiki.log 
        #可以输出文件xml,json,csv
        scrapy crawl apts -o output.json -t json
        scrapy crawl apts -o output.csv -t csv
        scrapy crawl apts -o output.xml -x xml 

4.  可以在settings.py 里面设定LOG_LEVEL.<p>
有５个LOG_LEVEL, 好熟悉的LEVEL, 跟linux的syslog倒是一脉相承.

        CRITICAL/ERROR/WARNING/DEBUG/INFO

5. 2017年12月整个一个月没有写一篇博客，惭愧，打算以后每５天，写一篇，争取每个月最少写６篇.
        